{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hi everyone! In this kernel, we will apply a stroke classification with ml models. The outline of the project:\n\n* [Introduction](#1)\n* [Background and Motivation](#2)\n* [Packages & Libraries](#3)\n* [First Look on the Data and Basic EDA](#4)\n* [Data Visualization](#5)\n* [Encoding, Missing Value Imputation and Oversampling](#6)\n    * [Encoding](#7)\n    * [Missing Value Imputation](#8)\n    * [Oversampling with Synthetic Minority Oversampling Technique (SMOTE)](#9)\n    * [Data Visualization After Oversampling](#10)\n    * [Splitting Data and Feature Scaling](#11)\n* [ML Models & Hyperparameter Tuning](#12)\n    * [Model Preparation](#13)\n    * [Hyperparameter Tuning](#14)\n    * [Determined Models with Tuned Parameters](#15)","metadata":{}},{"cell_type":"markdown","source":"<a id = \"1\"></a>\n### Introduction\n<img src=\"https://www.cdc.gov/stroke/images/Stroke-Medical-Illustration.jpg?_=77303?noicon\">","metadata":{}},{"cell_type":"markdown","source":"<a id = \"2\"></a>\n### Background and Motivation\n\nBackground: A stroke, sometimes called a brain attack, occurs when something blocks blood supply to part of the brain or when a blood vessel in the brain bursts. Brain cells begin to die in minutes. A stroke is a medical emergency, and prompt treatment is crucial. A stroke can cause lasting brain damage, long-term disability, or even death. Early action can reduce brain damage and other complications.\n\nMotivation: Our objective is to understand what are the reasons that cause stroke to people and see if we can succesfully detect stroke on the features from given data using ML techniques.","metadata":{}},{"cell_type":"markdown","source":"<a id = \"3\"></a>\n### Packages & Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report, roc_curve, precision_recall_curve, auc, confusion_matrix, roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.impute import KNNImputer\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-29T12:40:42.441152Z","iopub.execute_input":"2022-10-29T12:40:42.441658Z","iopub.status.idle":"2022-10-29T12:40:42.451903Z","shell.execute_reply.started":"2022-10-29T12:40:42.441609Z","shell.execute_reply":"2022-10-29T12:40:42.450792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"4\"></a>\n### First Look on the Data and Basic EDA","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\")\ndf = data.copy()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:42.453869Z","iopub.execute_input":"2022-10-29T12:40:42.454157Z","iopub.status.idle":"2022-10-29T12:40:42.49607Z","shell.execute_reply.started":"2022-10-29T12:40:42.454131Z","shell.execute_reply":"2022-10-29T12:40:42.49486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop unnecessary columns\ndf.drop([\"id\"], axis = 1, inplace = True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:42.497665Z","iopub.execute_input":"2022-10-29T12:40:42.498691Z","iopub.status.idle":"2022-10-29T12:40:42.518439Z","shell.execute_reply.started":"2022-10-29T12:40:42.498644Z","shell.execute_reply":"2022-10-29T12:40:42.517221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data info: columns with data types\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:42.522607Z","iopub.execute_input":"2022-10-29T12:40:42.52509Z","iopub.status.idle":"2022-10-29T12:40:42.540033Z","shell.execute_reply.started":"2022-10-29T12:40:42.525053Z","shell.execute_reply":"2022-10-29T12:40:42.538785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bmi column seems it has null values. Let's check it out!\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:42.542453Z","iopub.execute_input":"2022-10-29T12:40:42.543608Z","iopub.status.idle":"2022-10-29T12:40:42.555021Z","shell.execute_reply.started":"2022-10-29T12:40:42.543562Z","shell.execute_reply":"2022-10-29T12:40:42.553696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How many target variables of both \"stroke\" or \"not stroke\" classes on the dataset?","metadata":{}},{"cell_type":"code","source":"data = df[\"stroke\"].value_counts()\nlabels = df[\"stroke\"].value_counts().index\n\npalette_color = sns.color_palette('bright')\nplt.pie(data, labels=labels, colors=palette_color, autopct='%.0f%%')\nplt.title(\"The percentage of stroke 1: stroke, 0: non-stroke\");","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:42.556413Z","iopub.execute_input":"2022-10-29T12:40:42.556959Z","iopub.status.idle":"2022-10-29T12:40:42.679902Z","shell.execute_reply.started":"2022-10-29T12:40:42.556926Z","shell.execute_reply":"2022-10-29T12:40:42.677812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Issues:\nThere is 201 null BMI values on the dataset. Additionally the data is imbalance therefore we need to solve these issues for better results.","metadata":{}},{"cell_type":"markdown","source":"<a id = \"5\"></a>\n### Data Visualization","metadata":{}},{"cell_type":"code","source":"# a short look into the number of each categorical features grouped by stroke variable.\nsns.set_theme(style = 'darkgrid')\nfor i in df.columns[:-1]:  # exclude stroke column\n    if (df[i].dtype == 'object') or (df[i].dtype == 'int64'):\n            sns.countplot(data = df, x = i, hue = 'stroke')\n            plt.title('The number of the samples with {} based on stroke'.format(i))\n            plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:42.682347Z","iopub.execute_input":"2022-10-29T12:40:42.68387Z","iopub.status.idle":"2022-10-29T12:40:44.23369Z","shell.execute_reply.started":"2022-10-29T12:40:42.683803Z","shell.execute_reply":"2022-10-29T12:40:44.232594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a short look into numeric variables like bmi, avg_glucose_level and age\nsns.set_theme(style = 'darkgrid')\nfor i in df.columns[:-1]: # exclude stroke column\n    if df[i].dtype == 'float64':\n            sns.displot(data = df, x = i, hue = 'stroke', kde = True)\n            plt.title('The distribution of the {} based on stroke'.format(i))\n            plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:44.235037Z","iopub.execute_input":"2022-10-29T12:40:44.235442Z","iopub.status.idle":"2022-10-29T12:40:46.684808Z","shell.execute_reply.started":"2022-10-29T12:40:44.235413Z","shell.execute_reply":"2022-10-29T12:40:46.683554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation heatmap\nsns.heatmap(df.corr(), annot = True, cmap = 'crest')","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:46.6861Z","iopub.execute_input":"2022-10-29T12:40:46.686415Z","iopub.status.idle":"2022-10-29T12:40:47.166782Z","shell.execute_reply.started":"2022-10-29T12:40:46.686386Z","shell.execute_reply":"2022-10-29T12:40:47.165807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Violin plot to visualize each numerical variables by stroke\nplt.figure(figsize = (12, 7))\nsns.violinplot(data = df, x = \"work_type\", y=\"avg_glucose_level\", hue = 'stroke', inner = 'stick')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:47.170005Z","iopub.execute_input":"2022-10-29T12:40:47.170337Z","iopub.status.idle":"2022-10-29T12:40:55.674944Z","shell.execute_reply.started":"2022-10-29T12:40:47.170306Z","shell.execute_reply":"2022-10-29T12:40:55.673909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"6\"></a>\n### Encoding, Missing Value Imputation and Oversampling","metadata":{}},{"cell_type":"markdown","source":"<a id = \"7\"></a>\n#### Encoding","metadata":{}},{"cell_type":"code","source":"# First we need to transform our columns to be encoded to numpy arrays\ngender = df.iloc[:,0:1].values\never_married = df.iloc[:,4].values  \nwork_type = df.iloc[:,5:6].values\nresidence_type = df.iloc[:,6].values \nsmoking_status = df.iloc[:,9:10].values","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:55.676568Z","iopub.execute_input":"2022-10-29T12:40:55.676878Z","iopub.status.idle":"2022-10-29T12:40:55.684078Z","shell.execute_reply.started":"2022-10-29T12:40:55.676849Z","shell.execute_reply":"2022-10-29T12:40:55.683117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Other variables\nage = df[[\"age\"]]\nhypertension = df[[\"hypertension\"]]\nheart_disease = df[[\"heart_disease\"]]\navg_glucose_level = df[[\"avg_glucose_level\"]]\nbmi = df[[\"bmi\"]]\nstroke = df[[\"stroke\"]]","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:55.686068Z","iopub.execute_input":"2022-10-29T12:40:55.687044Z","iopub.status.idle":"2022-10-29T12:40:55.69913Z","shell.execute_reply.started":"2022-10-29T12:40:55.687011Z","shell.execute_reply":"2022-10-29T12:40:55.698116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique, counts = np.unique(ever_married, return_counts = True)\nprint(np.asarray((unique, counts)).T)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:55.702799Z","iopub.execute_input":"2022-10-29T12:40:55.703111Z","iopub.status.idle":"2022-10-29T12:40:55.715132Z","shell.execute_reply.started":"2022-10-29T12:40:55.703082Z","shell.execute_reply":"2022-10-29T12:40:55.714038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label Encoding for ever_married and residence_type columns which has two labels\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\never_married = le.fit_transform(ever_married)\never_married = pd.DataFrame(ever_married, columns = [\"ever_married\"])\nprint(\"Labels 0, 1:\",le.classes_)\nresidence_type = le.fit_transform(residence_type)\nresidence_type = pd.DataFrame(residence_type, columns = [\"residence_type\"])\nprint(\"Labels 0, 1:\",le.classes_)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:55.716937Z","iopub.execute_input":"2022-10-29T12:40:55.717793Z","iopub.status.idle":"2022-10-29T12:40:55.729603Z","shell.execute_reply.started":"2022-10-29T12:40:55.717748Z","shell.execute_reply":"2022-10-29T12:40:55.728462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One Hot Encoding for gender, work_type and smoking status columns\nohe = OneHotEncoder(dtype = np.int64, sparse = False)\ngender = ohe.fit_transform(gender)\ngender = pd.DataFrame(gender, columns = ['female', 'male', 'other'])\nprint(\"Gender dummies respectively 0, 1, 2:\", ohe.categories_)\nwork_type = ohe.fit_transform(work_type)\nwork_type = pd.DataFrame(work_type, columns = ['govt_job', 'never_worked', 'private', 'self-employed', 'children'])\nprint(\"Work type dummies respectively 0, 1, 2, 3, 4:\", ohe.categories_)\nsmoking_status = ohe.fit_transform(smoking_status)\nsmoking_status = pd.DataFrame(smoking_status, columns = ['unknown', 'formerly_smoked', 'never_smoked', 'smokes'])\nprint(\"Smoking status dummies respectively 0, 1, 2, 3:\", ohe.categories_)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:55.731227Z","iopub.execute_input":"2022-10-29T12:40:55.731677Z","iopub.status.idle":"2022-10-29T12:40:55.753571Z","shell.execute_reply.started":"2022-10-29T12:40:55.731638Z","shell.execute_reply":"2022-10-29T12:40:55.752428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frames = [gender, age, hypertension, heart_disease, ever_married, work_type, residence_type, avg_glucose_level, bmi, smoking_status, stroke]\ndf_en = pd.concat(frames, axis = 1)\ndf_en.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:55.755036Z","iopub.execute_input":"2022-10-29T12:40:55.755319Z","iopub.status.idle":"2022-10-29T12:40:55.77477Z","shell.execute_reply.started":"2022-10-29T12:40:55.755294Z","shell.execute_reply":"2022-10-29T12:40:55.773561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"8\"></a>\n#### Missing Value Imputation","metadata":{}},{"cell_type":"code","source":"# Missing value solution with Simple Imputer\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy = 'mean')\ndf_en = pd.DataFrame(imputer.fit_transform(df_en), columns = df_en.columns)\ndf_en.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:55.776299Z","iopub.execute_input":"2022-10-29T12:40:55.776789Z","iopub.status.idle":"2022-10-29T12:40:55.814736Z","shell.execute_reply.started":"2022-10-29T12:40:55.776755Z","shell.execute_reply":"2022-10-29T12:40:55.813855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"int_vars = [\"female\",\"male\",\"other\",\"age\",\"hypertension\",\"heart_disease\",\"ever_married\",\"govt_job\",\"never_worked\",\"private\",\"self-employed\", \n            \"children\", \"residence_type\",\"unknown\",\"formerly_smoked\",\"never_smoked\",\"smokes\"]\n\ndf1 = df_en[int_vars].astype(np.int64)\nnum_vars = [\"avg_glucose_level\",\"bmi\"]\ndf2 = df_en[num_vars]\nlabel = [\"stroke\"]\ndf3 = df_en[label]\ndf_en = pd.concat([df1, df2, df3], axis = 1)\ndf_en","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:55.81606Z","iopub.execute_input":"2022-10-29T12:40:55.81672Z","iopub.status.idle":"2022-10-29T12:40:55.846614Z","shell.execute_reply.started":"2022-10-29T12:40:55.816684Z","shell.execute_reply":"2022-10-29T12:40:55.845324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_en.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:55.847886Z","iopub.execute_input":"2022-10-29T12:40:55.848189Z","iopub.status.idle":"2022-10-29T12:40:55.860746Z","shell.execute_reply.started":"2022-10-29T12:40:55.848161Z","shell.execute_reply":"2022-10-29T12:40:55.85941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"9\"></a>\n#### Oversampling with Synthetic Minority Oversampling Technique (SMOTE)","metadata":{}},{"cell_type":"markdown","source":"Imbalanced classification involves developing predictive models on classification datasets that have a severe class imbalance.\n\nThe challenge of working with imbalanced datasets is that most machine learning techniques will ignore, and in turn have poor performance on, the minority class, although typically it is performance on the minority class that is most important.\n\nOne approach to addressing imbalanced datasets is to oversample the minority class. The simplest approach involves duplicating examples in the minority class, although these examples donâ€™t add any new information to the model. Instead, new examples can be synthesized from the existing examples. This is a type of data augmentation for the minority class and is referred to as the **Synthetic Minority Oversampling Technique,** or **SMOTE** for short.\n\nSource: [https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/]","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nX, y = df_en.iloc[:, 0:-1], df_en.iloc[:, -1:]\n\nprint(\"Before Oversampling, the counts of label 1: \", y.value_counts()[1])\nprint(\"Before Oversampling, the counts of label 0: \", y.value_counts()[0])\n\noversample = SMOTE()\nX, y = oversample.fit_resample(X, y)\n\nprint(\"After Oversampling, the counts of label 1: \", y.value_counts()[1])\nprint(\"After Oversampling, the counts of label 0: \", y.value_counts()[0])","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:55.862497Z","iopub.execute_input":"2022-10-29T12:40:55.863264Z","iopub.status.idle":"2022-10-29T12:40:55.908559Z","shell.execute_reply.started":"2022-10-29T12:40:55.86323Z","shell.execute_reply":"2022-10-29T12:40:55.906979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"10\"></a>\n#### Data Visualization After Oversampling","metadata":{}},{"cell_type":"code","source":"# visualization after SMOTE\ndf_as = pd.concat([X, y], axis = 1)\n\n# a short look into the number of each categorical features grouped by stroke variable.\nsns.set_theme(style = 'darkgrid')\nfor i in df_as.columns[:-1]:  # exclude stroke column\n    if (df_as[i].dtype == 'object') or (df_as[i].dtype == 'int64'):\n            sns.countplot(data = df_as, x = i, hue = 'stroke')\n            plt.title('The number of the samples with {} based on stroke'.format(i))\n            plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:40:55.910989Z","iopub.execute_input":"2022-10-29T12:40:55.91183Z","iopub.status.idle":"2022-10-29T12:41:00.431279Z","shell.execute_reply.started":"2022-10-29T12:40:55.911794Z","shell.execute_reply":"2022-10-29T12:41:00.429958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can easily see that stroke 1 labels are increased **after SMOTE**","metadata":{}},{"cell_type":"code","source":"sns.set_theme(style = 'darkgrid')\nfor i in df_as.columns[:-1]: # exclude stroke column\n    if df_as[i].dtype == 'float64':\n            sns.displot(data = df_as, x = i, hue = 'stroke', kde = True)\n            plt.title('The distribution of the {} based on stroke'.format(i))\n            plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:41:00.433227Z","iopub.execute_input":"2022-10-29T12:41:00.433687Z","iopub.status.idle":"2022-10-29T12:41:02.5219Z","shell.execute_reply.started":"2022-10-29T12:41:00.433644Z","shell.execute_reply":"2022-10-29T12:41:02.520804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"11\"></a>\n#### Splitting Data and Feature Scaling","metadata":{}},{"cell_type":"code","source":"# Splitting data to validation and training\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n# Scaling features between -1 and 1\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:41:02.523844Z","iopub.execute_input":"2022-10-29T12:41:02.524299Z","iopub.status.idle":"2022-10-29T12:41:02.546174Z","shell.execute_reply.started":"2022-10-29T12:41:02.524237Z","shell.execute_reply":"2022-10-29T12:41:02.545156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"12\"></a>\n### ML Models & Hyperparameter Tuning","metadata":{}},{"cell_type":"markdown","source":"<a id = \"13\"></a>\n#### Model Preparation","metadata":{}},{"cell_type":"code","source":"# Models to be used for ML\nmodels = [('Logistic Regression', LogisticRegression()),\n          ('Decision Tree Classifier', DecisionTreeClassifier()),\n          ('Random Forest', RandomForestClassifier()),\n          ('Linear Discriminant Analyzer', LinearDiscriminantAnalysis()),\n          ('Ada Boost', AdaBoostClassifier()),\n          ('KNN', KNeighborsClassifier()),\n          ('Support Vector Classifier', SVC(probability = True)),\n          ('XG Boost', XGBClassifier()),\n          ('Cat Boost', CatBoostClassifier(logging_level = 'Silent'))]\n\nmodels_score = []\nfor name, model in models:\n    model = model\n    model.fit(x_train, y_train)\n    model.predict(x_test)\n    models_score.append([name, accuracy_score(y_test, model.predict(x_test))])\n    \n    print(\"Model: \",name)\n    print('Validation Accuracy: ', accuracy_score(y_test, model.predict(x_test)))\n    print('Training Accuracy: ', accuracy_score(y_train, model.predict(x_train)))\n    \n    plt.figure()\n    cf_matrix = confusion_matrix(y_test, model.predict(x_test))\n    plt.title('Confusion Matrix: {}'.format(name))\n    sns.heatmap(cf_matrix, annot = True, fmt = 'g', cmap = sns.cubehelix_palette(as_cmap=True))\n    plt.show()\n    \n    import scikitplot as skplt\n\n    skplt.metrics.plot_roc(y_test, model.predict_proba(x_test))\n    plt.title('ROC Curves: {}'.format(name))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:41:02.547624Z","iopub.execute_input":"2022-10-29T12:41:02.548046Z","iopub.status.idle":"2022-10-29T12:41:27.476321Z","shell.execute_reply.started":"2022-10-29T12:41:02.548005Z","shell.execute_reply":"2022-10-29T12:41:27.475181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Barplot to show the test accuracy scores of each algorithms\nplt.figure(figsize = (12, 6))\nsns.barplot(x = np.array(models_score)[:, 0], y=np.array(models_score)[:, 1].astype('float64'))\nplt.xticks(rotation = 45);","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:41:27.480787Z","iopub.execute_input":"2022-10-29T12:41:27.481555Z","iopub.status.idle":"2022-10-29T12:41:27.798005Z","shell.execute_reply.started":"2022-10-29T12:41:27.481489Z","shell.execute_reply":"2022-10-29T12:41:27.796754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"14\"></a>\n#### Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"# Models with hyperparameters to be tuned\ngrid_models = [(LogisticRegression(),[{'C' : [0.3, 0.7, 1], 'random_state' : [42]}]),\n               (DecisionTreeClassifier(),[{'criterion' : ['gini','entropy'], 'random_state' : [42]}]),\n               (RandomForestClassifier(),[{'n_estimators' : [100, 200, 300], 'criterion' : ['gini','entropy'], 'random_state' : [42]}]),\n               (LinearDiscriminantAnalysis(),[{'solver' : ['svd', 'lsqr', 'eigen']}]),\n               (AdaBoostClassifier(),[{'n_estimators' : [50, 100, 150], 'random_state' : [42]}]),\n               (KNeighborsClassifier(),[{'n_neighbors' : [4, 6, 8, 10], 'metric' : ['euclidean', 'manhattan', 'chebyshev','minkowski']}]),\n               (SVC(),[{'C' : [0.3, 0.7, 1], 'kernel' : ['rbf','linear','polynomial'], 'random_state' : [42]}]),\n               (XGBClassifier(),[{'max_depth' : [3, 5, 7], 'min_child_weight' : [1, 3, 5]}]),\n               (CatBoostClassifier(),[{'n_estimators' : [100, 200, 300], 'max_depth' : [3,5,7]}])]","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:41:27.799643Z","iopub.execute_input":"2022-10-29T12:41:27.80012Z","iopub.status.idle":"2022-10-29T12:41:27.812829Z","shell.execute_reply.started":"2022-10-29T12:41:27.800074Z","shell.execute_reply":"2022-10-29T12:41:27.811417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model, param_grid  in grid_models:\n    cv = GridSearchCV(estimator = model, param_grid = param_grid, scoring = 'accuracy', cv = 5)\n    cv.fit(x_train, y_train)\n    best_accuracy = cv.best_score_\n    best_params = cv.best_params_\n    print('{}: \\nBest Accuracy: {:.2f}%'.format(model, best_accuracy*100))\n    print('Best Parameters: ',best_params)\n    print('******************************')","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:41:27.814398Z","iopub.execute_input":"2022-10-29T12:41:27.814898Z","iopub.status.idle":"2022-10-29T12:44:18.025409Z","shell.execute_reply.started":"2022-10-29T12:41:27.814851Z","shell.execute_reply":"2022-10-29T12:44:18.023441Z"},"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we expand the hide block and look at the results we can easily determine that the **Random Forest, XGBoost** and **Cat Boost** algorithms give the best scores after tuning.","metadata":{}},{"cell_type":"markdown","source":"<a id = \"15\"></a>\n#### Determined Models with Tuned Parameters","metadata":{}},{"cell_type":"code","source":"# The best three models with the best chosen parameters after tuning\nmodels = [('Random Forest', RandomForestClassifier(criterion = 'gini', n_estimators = 100, random_state = 42)),\n          ('XG Boost', XGBClassifier(max_depth = 7, min_child_weight = 1)),\n          ('Cat Boost', CatBoostClassifier(max_depth = 7, n_estimators = 300, logging_level = 'Silent'))]\n\nmodels_score = []\nfor name, model in models:\n    model = model\n    model.fit(x_train, y_train)\n    model.predict(x_test)\n    models_score.append([name, accuracy_score(y_test, model.predict(x_test))])\n    \n    print(\"Model: \",name)\n    print('Validation Accuracy: ', accuracy_score(y_test, model.predict(x_test)))\n    print('Training Accuracy: ', accuracy_score(y_train, model.predict(x_train)))\n    \n    plt.figure()\n    cf_matrix = confusion_matrix(y_test, model.predict(x_test))\n    plt.title('Confusion Matrix: Tuned {}'.format(name))\n    sns.heatmap(cf_matrix, annot = True, fmt = 'g', cmap = sns.cubehelix_palette(as_cmap=True))\n    plt.show()\n    \n    import scikitplot as skplt\n\n    skplt.metrics.plot_roc(y_test, model.predict_proba(x_test))\n    plt.title('ROC Curves: Tuned {}'.format(name))\n    plt.show()\n    \n    importance = model.feature_importances_\n    # summarize feature importance\n    for i,v in enumerate(importance):\n        print('Feature: %0d, Score: %.5f' % (i,v))\n    # plot feature importance\n    plt.figure(figsize = (12, 5))\n    plt.bar([x for x in range(len(importance))], importance)\n    plt.title(\"{} Classification Feature Importance\".format(name))\n    plt.xticks(range(0, 19))\n    plt.show()\n    print('')\n    print('#######################################################')\n    print('#######################################################')\n    print('')","metadata":{"execution":{"iopub.status.busy":"2022-10-29T12:44:18.027241Z","iopub.execute_input":"2022-10-29T12:44:18.027619Z","iopub.status.idle":"2022-10-29T12:44:24.859959Z","shell.execute_reply.started":"2022-10-29T12:44:18.027585Z","shell.execute_reply":"2022-10-29T12:44:24.858546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Thank you!\nPlease upvote if you liked this work...**","metadata":{"_kg_hide-output":true}}]}